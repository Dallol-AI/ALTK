{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Tokenizer Algorithm\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, delimiters, include_punc=False):\n",
    "    \"\"\"\n",
    "    Tokenize a text into a list of tokens.\n",
    "\n",
    "    :param text: the text to tokenize\n",
    "    :param delimiters: the delimiters to use\n",
    "    :param include_punc: include punctuation in the tokens\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = []\n",
    "    curr_word = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        if char not in delimiters:\n",
    "            curr_word += char\n",
    "        elif curr_word.strip():\n",
    "            tokens.append(curr_word.strip())\n",
    "            if include_punc and char != ' ': tokens.append(char)\n",
    "            curr_word = \"\"\n",
    "            \n",
    "    if curr_word.strip(): # if a word exists and is not whitespace\n",
    "        tokens.append(curr_word.strip())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_delimiters = { \"[\", \" \", \"፣\", \"።\", \",\", \"፦\", \"!\", \">\", \"&\", \"፧\", \"}\", \"^\", \")\", \"፨\", \"<\", \"~\", \"]\", \"-\", \"*\", \"{\", \"፡\", \"፤\", \"/\", \"፥\", \"(\", \"\\\\\", \"_\", \"+\", \";\", \"#\", \"\\\"\", \":\", \"=\", \" \", \"%\", \"|\", \"`\", \"@\", \"'\", \"?\", \"$\", }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "ቢግ ማክ \n",
    "የሃምበርገር ዓይነት ሲሆን በፈጣን ምግብ ቤቱ ማክዶናልድስ የሚሸጥ ነው። \n",
    "    ሃምበርገሩ ለመጀመሪያ ጊዜ የተፈጠረው በ1960 ዓ.ም. በአሜሪካኑ ጅም ዴልጋቲ ነበር። ሁለት የተፈጨ የበሬ ስጋ ክቦችን፣ ሰላጣ ቅጠል፣ ዓይብ፣ ሽንኩርት፣ ፒክልስ እና ሶስት የሰሊጥ ጠፍጣፋ ዳቦዎችን ከማዋዣ የቢግ ማክ ሶስ (መረቅ) ጋር ይይዛል።\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ቢግ',\n",
       " 'ማክ',\n",
       " 'የሃምበርገር',\n",
       " 'ዓይነት',\n",
       " 'ሲሆን',\n",
       " 'በፈጣን',\n",
       " 'ምግብ',\n",
       " 'ቤቱ',\n",
       " 'ማክዶናልድስ',\n",
       " 'የሚሸጥ',\n",
       " 'ነው',\n",
       " '።',\n",
       " 'ሃምበርገሩ',\n",
       " 'ለመጀመሪያ',\n",
       " 'ጊዜ',\n",
       " 'የተፈጠረው',\n",
       " 'በ1960',\n",
       " 'ዓ.ም.',\n",
       " 'በአሜሪካኑ',\n",
       " 'ጅም',\n",
       " 'ዴልጋቲ',\n",
       " 'ነበር',\n",
       " '።',\n",
       " 'ሁለት',\n",
       " 'የተፈጨ',\n",
       " 'የበሬ',\n",
       " 'ስጋ',\n",
       " 'ክቦችን',\n",
       " '፣',\n",
       " 'ሰላጣ',\n",
       " 'ቅጠል',\n",
       " '፣',\n",
       " 'ዓይብ',\n",
       " '፣',\n",
       " 'ሽንኩርት',\n",
       " '፣',\n",
       " 'ፒክልስ',\n",
       " 'እና',\n",
       " 'ሶስት',\n",
       " 'የሰሊጥ',\n",
       " 'ጠፍጣፋ',\n",
       " 'ዳቦዎችን',\n",
       " 'ከማዋዣ',\n",
       " 'የቢግ',\n",
       " 'ማክ',\n",
       " 'ሶስ',\n",
       " 'መረቅ',\n",
       " ')',\n",
       " 'ጋር',\n",
       " 'ይይዛል',\n",
       " '።']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sample_text, amharic_delimiters, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from typing import List\n",
    "from utils import apply_rules, RULES\n",
    "\n",
    "# TODO: HANDLE COMPOUND WORDS SUCH AS ስነ\n",
    "\n",
    "class AmharicTokenizer:\n",
    "    def __init__(self, word_delimiters: set = None, sentence_delimiters: set = None):\n",
    "        \"\"\"\n",
    "        Amharic Tokenizer for tokenization and sentence segmentation.\n",
    "\n",
    "        Args:\n",
    "        - sentence_punctuations (Set[str]): List of sentence-ending punctuations.\n",
    "        - word_punctuations (Set[str]): List of word-ending punctuations.\n",
    "        \"\"\"\n",
    "        self.__word_delimiters = word_delimiters or { \"[\", \" \", \"፣\", \"።\", \",\", \"፦\", \"!\", \">\", \"&\", \"፧\", \"}\", \"^\", \")\", \"፨\", \"<\", \"~\", \"]\", \"*\", \"{\", \"፤\", \"/\", \"፥\", \"(\", \"\\\\\", \"_\", \"+\", \";\", \"#\", \"\\\"\", \":\", \"=\", \" \", \"%\", \"|\", \"`\", \"@\", \"'\", \"?\", \"$\", }\n",
    "        self.__sentence_delimiters = sentence_delimiters or [\"።\", \"፥\", \"፨\", \"::\", \"፡፡\", \"?\", \"!\",'፧']\n",
    "        self.__compound_words_fix = [\n",
    "            'ስነ','ቤተ', 'እግረ','ሥነ'\n",
    "        ]\n",
    "\n",
    "    def word_tokenize(self, text: str, include_punc=False, compound_words_as_one=True, clean=False):\n",
    "        \"\"\"\n",
    "        Tokenize a text into a list of tokens.\n",
    "\n",
    "        :param text: the text to tokenize\n",
    "        :param include_punc: include punctuation in the tokens\n",
    "        :param clean: apply basic cleaning rules to the text\n",
    "        :return: a list of tokens\n",
    "        \"\"\"\n",
    "        if clean: text = apply_rules(text, RULES)\n",
    "        \n",
    "        delimiters = self.__word_delimiters\n",
    "        compound_words = self.__compound_words_fix\n",
    "        \n",
    "        tokens = []\n",
    "        curr_word = \"\"\n",
    "        prev_word = None\n",
    "        \n",
    "        for char in text:\n",
    "            if char not in delimiters:\n",
    "                curr_word += char\n",
    "            else:\n",
    "                curr_word = curr_word.strip()\n",
    "                if curr_word:\n",
    "                    if compound_words_as_one and curr_word in self.__compound_words_fix:\n",
    "                        continue\n",
    "                    tokens.append(curr_word)\n",
    "                    prev_word = curr_word\n",
    "                    if include_punc and char != ' ': tokens.append(char);prev_word = char;\n",
    "                    curr_word = \"\"\n",
    "                \n",
    "                \n",
    "        if curr_word.strip(): # if a word exists and is not whitespace\n",
    "            tokens.append(curr_word.strip())\n",
    "        return tokens\n",
    "\n",
    "    def sentence_tokenize(self, text: str, clean=True):\n",
    "        \"\"\"\n",
    "        Tokenize a text into a list of sentences.\n",
    "\n",
    "        :param text: the text to tokenize\n",
    "        :param clean: apply basic cleaning rules to the text\n",
    "        :return: a list of sentences\n",
    "        \"\"\"\n",
    "        if clean: text = apply_rules(text, RULES)\n",
    "                \n",
    "        sentences = []\n",
    "        current_sentence = \"\"\n",
    "    \n",
    "        for char in sentences:\n",
    "            if char not in self.__sentence_delimiters:\n",
    "                current_sentence += char\n",
    "            elif current_sentence.strip():\n",
    "                sentences.append(current_sentence.strip())\n",
    "                current_sentence = \"\"\n",
    "\n",
    "        if current_sentence.strip():\n",
    "            sentences.append(current_sentence.strip())\n",
    "        return sentences\n",
    "    \n",
    "    def matrix_tokenize(self, text: str, clean=False, include_punc=False, compound_words_as_one=False) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Tokenize text in to list of tokenized sentences.\n",
    "        \n",
    "        :param text: the text to tokenize\n",
    "        :param clean: apply basic cleaning rules to the text\n",
    "        :param include_punc: Include punctuations in the word tokens.\n",
    "        :param compound_words_as_one: Include compounds words in the word tokens as one word.\n",
    "                \n",
    "        :return: a list of word-tokenized sentences\n",
    "        \"\"\"\n",
    "        if clean:\n",
    "            text = apply_rules(text)\n",
    "        \n",
    "        matrix = []\n",
    "        curr_row = []\n",
    "        curr_word = \"\"\n",
    "        prev_word = None\n",
    "        \n",
    "        for char in text:\n",
    "            if char not in self.__sentence_delimiters and char not in self.__word_delimiters:\n",
    "                curr_word += char\n",
    "            elif char in self.__word_delimiters:\n",
    "                curr_word = curr_word.strip()\n",
    "                if curr_word:\n",
    "                    if compound_words_as_one and curr_word in self.__compound_words_fix:\n",
    "                        continue\n",
    "                    curr_row.append(curr_word)\n",
    "                    prev_word = curr_word\n",
    "                    if include_punc and char != ' ': tokens.append(char);prev_word = char;\n",
    "                    curr_word = \"\"\n",
    "                if char in self.sentence_delimiters:\n",
    "                    matrix.append(curr_row)\n",
    "                    curr_row = []\n",
    "                    \n",
    "                    \n",
    "        if current_word.strip(): curr_row.append(current_word.strip())\n",
    "        if current_row: matrix.append(current_row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def __find_indexes(text, punct):\n",
    "        \"\"\"\n",
    "        returns the index of the sentence delimiters in the text\n",
    "        \"\"\"\n",
    "        return [i + len(punct) - 1 for i in range(len(text)) if text.startswith(punct, i)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AmharicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amharic_cleaner import clean_amharic_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
